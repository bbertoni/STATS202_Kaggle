---
title: "kaggle"
output: html_document
---


## Libraries
```{r}
library(dplyr)
library(glmnet)
library(pls)
```


## Data cleaning
```{r}
clean_data = function(df){
  # remove ID variable since it just labels the rows
  # remove surface area because it is equal to wall area + 2 *(roof area)
  df = df[,-c(1,3)]
  # change orientation variables to categorical variables
  df$Orientation = as.factor(df$Orientation)
  return(df)
}

train = read.csv("~/Documents/Courses/Stats 202/Kaggle Competition/train.csv")
train = clean_data(train)
test = read.csv("~/Documents/Courses/Stats 202/Kaggle Competition/test.csv")
test = clean_data(test)

sample_submission = read.csv("~/Documents/Courses/Stats 202/Kaggle Competition/sampleSubmission.csv")
```


## Linear Model with Regularization

O. Linear Regression
```{r}
linear.mod = lm(Outcome~.,data=train)
mean(linear.mod$residuals) # MSE = 8.810815
```

1. Ridge Regression
```{r}
# transform data
set.seed(101)
train.mat = model.matrix(Outcome~.,data=train)[,-1]
test.mat = test %>% mutate(Outcome = 0)
test.mat = model.matrix(Outcome~.,data=test.mat)[,-1]

# cross-validation to select the best lambda
grid=10^seq(10,-2,length=100)
cv.out=cv.glmnet(train.mat,train$Outcome,alpha=0,lambda = grid,thresh = 1e-12)
bestlam=cv.out$lambda.min
bestlam

# get MSE and predict results
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(train.mat,train$Outcome,alpha=0,lambda = grid,thresh = 1e-12)
ridge.train = predict(ridge.mod,s=bestlam,newx=train.mat)
mean((ridge.train-train$Outcome)^2) # MSE = 8.529983
ridge.test=predict(ridge.mod,s=bestlam,newx=test.mat)
ridge.result = as.data.frame(cbind(sample_submission$ID,ridge.test))
names(ridge.result)=c("ID","Outcome")
```

2. Lasso Regression
```{r}
set.seed(102)

# cross-validation to select the best lambda
grid=10^seq(10,-2,length=100)
cv.out=cv.glmnet(train.mat,train$Outcome,alpha=1,lambda = grid,thresh = 1e-12)
bestlam=cv.out$lambda.min
bestlam

# get MSE and predict results
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(train.mat,train$Outcome,alpha=1,lambda = grid,thresh = 1e-12)
lasso.train = predict(lasso.mod,s=bestlam,newx=train.mat)
mean((lasso.train-train$Outcome)^2) # MSE = 8.560334
lasso.test=predict(lasso.mod,s=bestlam,newx=test.mat)
lasso.result = as.data.frame(cbind(sample_submission$ID,ridge.test))
names(ridge.result)=c("ID","Outcome")
```


3. Principal Component Regression
```{r}
set.seed(103)

# cross-validation to select the optimal number of components
pcr.fit = pcr(Outcome~., data = train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="MSEP") # best M = 9
```

```{r}
# get MSE and predict results
pcr.train = predict(pcr.fit,train,ncomp=9)
mean((pcr.train - train$Outcome)^2) # MSE = 8.517407
pcr.test=predict(pcr.fit,test,ncomp=9)
pcr.result = as.data.frame(cbind(sample_submission$ID,pcr.test))
names(pcr.result)=c("ID","Outcome")
```

4. Partial Least Squares
```{r}
set.seed(104)

# cross-validation to select the optimal number of components
pls.fit = plsr(Outcome~., data = train,scale=TRUE,validation="CV")
validationplot(pls.fit,val.type="MSEP") # best M = 9
```

```{r}
# get MSE and predict results
pls.train = predict(pls.fit,train,ncomp=9)
mean((pls.train - train$Outcome)^2) # MSE = 8.517407
pls.test=predict(pls.fit,test,ncomp=9)
pls.result = as.data.frame(cbind(sample_submission$ID,pls.test))
names(pls.result)=c("ID","Outcome")
```

## Save Results
```{r}
# Ridge score: 3.04311
write.csv(ridge.result,
          "~/Documents/Courses/Stats 202/Kaggle Competition/test_ridge_2017_11_24.csv",
          row.names = F)

# Ridge score: 3.03155
write.csv(pcr.result,
          "~/Documents/Courses/Stats 202/Kaggle Competition/test_pcr_2017_11_24.csv",
          row.names = F)
```

